{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from autoencoder import model\n",
    "import pickle\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "PLOTTING_SUPPORT = True\n",
    "RUN_AS_PY_SCRIPT = False\n",
    "SET_EULER_PARAMS = False\n",
    "SET_MARMOT_PARAMS = False\n",
    "\n",
    "# Handle arguments (When executed as .py script)\n",
    "import sys\n",
    "argv = sys.argv[:]\n",
    "if len(argv) > 1:\n",
    "  script_path = argv.pop(0)\n",
    "  if \"--euler\" in argv:\n",
    "    import sys\n",
    "    sys.stdout = open('stdout.txt', 'w')\n",
    "    RUN_AS_PY_SCRIPT = True\n",
    "    PLOTTING_SUPPORT = False\n",
    "    SET_EULER_PARAMS = True\n",
    "    print(\"Parameters set for execution on euler cluster\")\n",
    "    argv.remove(\"--euler\")\n",
    "  if \"--marmot\" in argv:\n",
    "    RUN_AS_PY_SCRIPT = True\n",
    "    PLOTTING_SUPPORT = False\n",
    "    SET_MARMOT_PARAMS = True\n",
    "    print(\"Parameters set for execution on marmot cluster\")\n",
    "    argv.remove(\"--marmot\") \n",
    "  if \"--script\" in argv:\n",
    "    RUN_AS_PY_SCRIPT = True\n",
    "    PLOTTING_SUPPORT = False\n",
    "    print(\"Running as script\")\n",
    "    argv.remove(\"--script\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  %load_ext autoreload\n",
    "  %autoreload 2\n",
    "  from IPython.display import clear_output\n",
    "  if PLOTTING_SUPPORT:\n",
    "    %matplotlib notebook\n",
    "    from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "VOXEL_SIDE = 24\n",
    "\n",
    "MAX_STEPS = 10000\n",
    "VAL_EXAMPLES = 200\n",
    "N_ROTATION_ANGLES = 12\n",
    "ROTATION_OFFSET = 0\n",
    "VAL_EVERY_N_STEPS = 1\n",
    "VAL_STEP_TOLERANCE = 3\n",
    "ROTATE_SEGMENTS_EVERY_STEP = True\n",
    "G_THRESHOLD = 0.80\n",
    "D_THRESHOLD = 0.45\n",
    "\n",
    "MP = model.ModelParams()\n",
    "MP.INPUT_SHAPE = [VOXEL_SIDE, VOXEL_SIDE, VOXEL_SIDE, 1]\n",
    "\n",
    "HOME_DIR = os.path.expanduser('~')\n",
    "DATA_DIR = \"./database/\"\n",
    "RUN_NAME = \"kitti18\"\n",
    "RESTORE_MODEL = True\n",
    "SAVE_DIR = HOME_DIR + \"/Desktop/autoencoder/\"\n",
    "SAVE_FILE = \"model.checkpoint\"\n",
    "MP_FILENAME = \"model_params.pckl\"\n",
    "TENSORBOARD_DIR = \"/tmp/tf_log\"\n",
    "SAVE_DIR_NOVAL = \"/tmp/unvalidated/\"\n",
    "SAVE_UNVALIDATED = True\n",
    "CREATE_VISUALS = False\n",
    "DETAILED_STEP_TIMES = False\n",
    "\n",
    "EXPORT_FEATURES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if SET_EULER_PARAMS:\n",
    "    DATA_DIR = \"/cluster/home/dugasd/database/\"\n",
    "    SAVE_DIR = \"/cluster/home/dugasd/autoencoder-euler/\"\n",
    "    TENSORBOARD_DIR = None\n",
    "    CREATE_VISUALS = False\n",
    "    \n",
    "    MAX_STEPS = 1000000\n",
    "    VAL_STEP_TOLERANCE = 5\n",
    "    \n",
    "if SET_MARMOT_PARAMS:\n",
    "    DATA_DIR = \"/home/daniel/database/\"\n",
    "    RUN_NAME = \"kitti18-20-27\"\n",
    "    SAVE_DIR = \"/home/daniel/autoencoder-marmot/\"\n",
    "    SAVE_UNVALIDATED = True\n",
    "    TENSORBOARD_DIR = None\n",
    "    CREATE_VISUALS = False\n",
    "    \n",
    "    MAX_STEPS = 1000000\n",
    "    VAL_STEP_TOLERANCE = 10\n",
    "    \n",
    "if not RUN_AS_PY_SCRIPT:\n",
    "    MP.CONVOLUTION_LAYERS = []\n",
    "    CREATE_VISUALS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if RUN_AS_PY_SCRIPT:\n",
    "  while argv:\n",
    "      arg = argv.pop(0)\n",
    "      if arg == \"-RUN_NAME\":\n",
    "        RUN_NAME = argv.pop(0)\n",
    "        print(\"RUN_NAME set to \" + RUN_NAME)\n",
    "      elif arg == \"-SAVE_DIR\":\n",
    "        SAVE_DIR = argv.pop(0)\n",
    "        print(\"SAVE_DIR set to \" + SAVE_DIR)\n",
    "      elif arg == \"-BATCH_SIZE\":\n",
    "        BATCH_SIZE = int(argv.pop(0))\n",
    "        print(\"BATCH_SIZE set to \" + str(BATCH_SIZE))\n",
    "      elif arg == \"--noconv\":\n",
    "        MP.CONVOLUTION_LAYERS = []\n",
    "        print(\"CONVOLUTION LAYERS REMOVED\")\n",
    "      elif arg == \"--no_rotation\":\n",
    "        ROTATE_SEGMENTS_EVERY_STEP = False\n",
    "        print(\"Disabling segment rotation.\")\n",
    "      elif arg == \"-LEARNING_RATE\":\n",
    "        MP.LEARNING_RATE = float(argv.pop(0))\n",
    "        print(\"LEARNING_RATE set to \" + str(MP.LEARNING_RATE))\n",
    "      elif arg == \"-LATENT_SHAPE\":\n",
    "        MP.LATENT_SHAPE = [int(argv.pop(0))]\n",
    "        print(\"LATENT_SHAPE set to \" + str(MP.LATENT_SHAPE))\n",
    "      elif arg == \"-VAL_STEP_TOLERANCE\":\n",
    "        VAL_STEP_TOLERANCE = int(argv.pop(0))\n",
    "        print(\"VAL_STEP_TOLERANCE set to \" + str(VAL_STEP_TOLERANCE))\n",
    "      elif arg == \"-ROTATION_OFFSET\":\n",
    "        frac = list(map(float, argv.pop(0).split('/'))) + [1.0]\n",
    "        ROTATION_OFFSET = frac[0]/frac[1]\n",
    "        print(\"ROTATION_OFFSET set to \" + str(ROTATION_OFFSET))\n",
    "      elif arg == \"--float64\":\n",
    "        MP.FLOAT_TYPE = tf.float64\n",
    "        print(\"MP.FLOAT_TYPE set to \" + str(MP.FLOAT_TYPE))\n",
    "      elif arg == \"--not-vegan\":\n",
    "        MP.ADVERSARIAL = False\n",
    "        MP.MUTUAL_INFO = False\n",
    "        print(\"Reverting to variational autoencoder model only.\")\n",
    "      elif arg == \"--vegan\":\n",
    "        MP.ADVERSARIAL = True\n",
    "        MP.MUTUAL_INFO = True\n",
    "        print(\"Enabling adversarial and mutual info graphs.\")\n",
    "      else:\n",
    "        print(\"Unknown argument: \" + arg)\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = SAVE_DIR+SAVE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Segments and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utilities\n",
    "run_names, runs = utilities.list_runs(DATA_DIR)\n",
    "try:\n",
    "  run_names.remove(RUN_NAME)\n",
    "  run_names = [RUN_NAME] + run_names\n",
    "except:\n",
    "  print(RUN_NAME + \" not found in runs.\")\n",
    "print(run_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  from ipywidgets import widgets\n",
    "  run_dropdown = widgets.Dropdown(description=\"Run to import : \", options=run_names)\n",
    "  button = widgets.Button(description=\"import\")\n",
    "\n",
    "  # Interaction functions\n",
    "  def import_run_data(btn):\n",
    "    display.clear_output()\n",
    "    print(\"Loading segments, features, matches, classes for run\")\n",
    "    global segments, features, fnames, matches, classes, ids, classes_set # 'output' variables\n",
    "    segments, features, fnames, matches, classes, ids = utilities.import_run(run_dropdown.value, folder=DATA_DIR)\n",
    "    classes_set = sorted(list(set(classes)))\n",
    "  \n",
    "  button.on_click(import_run_data)\n",
    "  # Display widgets\n",
    "  from IPython import display\n",
    "  display.display(run_dropdown)\n",
    "  display.display(button)\n",
    "\n",
    "  import_run_data(button)\n",
    "else:\n",
    "  segments, features, fnames, matches, classes, ids = utilities.import_run(RUN_NAME, folder=DATA_DIR)\n",
    "  classes_set = sorted(list(set(classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  try:\n",
    "    stored_MP = pickle.load(open(SAVE_DIR+MP_FILENAME, 'rb'))\n",
    "    if MP != stored_MP: \n",
    "        print(\"WARNING: Setting params for compatibility with stored model.\")\n",
    "        print(\"Stored model: \"); print(stored_MP); print(\"New model: \"); print(MP)\n",
    "    MP = stored_MP\n",
    "  except FileNotFoundError:\n",
    "    print(\"No stored model found. Creating a new model.\")\n",
    "temp = MP.DISABLE_SUMMARY\n",
    "MP.DISABLE_SUMMARY = True if TENSORBOARD_DIR is None else False\n",
    "if MP.DISABLE_SUMMARY != temp: print(\"Summary\", \"enabled\" if not MP.DISABLE_SUMMARY else \"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = model.Autoencoder(MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_writer = None\n",
    "if TENSORBOARD_DIR != None:\n",
    "  summary_writer = tf.train.SummaryWriter(TENSORBOARD_DIR, vae.sess.graph)\n",
    "  print(\"Graph written to log.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if RESTORE_MODEL:\n",
    "  try:\n",
    "    vae.saver.restore(vae.sess, SAVE_PATH)\n",
    "    print(\"Model restored.\")\n",
    "    print(MP.CONVOLUTION_LAYERS)\n",
    "  except Exception as err:\n",
    "    print(\"Could not load model: \", end=\"\")\n",
    "    try:\n",
    "      stored_MP = pickle.load(open(SAVE_DIR+MP_FILENAME, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        print(\"no model folder.\")\n",
    "    else:\n",
    "      print(\"ERROR: mismatch between model params.\")\n",
    "      print(\"Stored model: \"); print(stored_MP); print(\"New model: \"); print(MP)\n",
    "      raise err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Voxelized Segment Dataset - With Rotated Copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split into training and val data\n",
    "split_at = min(VAL_EXAMPLES, int(0.2 * len(ids)))\n",
    "val = segments[:split_at]\n",
    "train = segments[split_at:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelize import voxelize\n",
    "if not ROTATE_SEGMENTS_EVERY_STEP:\n",
    "  print(\"Voxelizing training data\")\n",
    "  train_vox, _ = voxelize(train,VOXEL_SIDE)\n",
    "\n",
    "  del train # Save some memory\n",
    "val_vox, _   = voxelize(val  ,VOXEL_SIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Using \" + str(process.memory_info().rss/(1024.0*1024.0)) + \"mB of memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder ( Computationally Intensive )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from dynamic_average import Average\n",
    "from autoencoder.batchmaker import Batchmaker, progress_bar\n",
    "\n",
    "avg_step_cost = None\n",
    "step_cost_log = []\n",
    "val_steps_since_last_improvement = 0\n",
    "step_start = timer()\n",
    "\n",
    "try:\n",
    "    val_cost_log = list(np.loadtxt(SAVE_DIR+\"val_cost_log.txt\"))\n",
    "    print(\"Previous cost log found.\")\n",
    "except:\n",
    "    val_cost_log = []\n",
    "    \n",
    "# single step\n",
    "try:\n",
    "    for step in range(MAX_STEPS):\n",
    "      if ROTATE_SEGMENTS_EVERY_STEP:\n",
    "          from voxelize import random_rotated\n",
    "          val = random_rotated(val)\n",
    "          train = random_rotated(train)\n",
    "          from voxelize import voxelize\n",
    "          train_vox, _ = voxelize(train,VOXEL_SIDE)\n",
    "          val_vox, _   = voxelize(val ,VOXEL_SIDE)  # Validation\n",
    "      val_batchmaker = Batchmaker(val_vox, BATCH_SIZE, MP)\n",
    "      if np.mod(step, VAL_EVERY_N_STEPS) == 0:\n",
    "        avg_val_cost = Average()\n",
    "        while True:\n",
    "          if val_batchmaker.is_depleted():\n",
    "            break\n",
    "          else:\n",
    "            batch_input_values = val_batchmaker.next_batch()\n",
    "            cost_value = vae.cost_on_single_batch(batch_input_values, summary_writer=summary_writer)\n",
    "            avg_val_cost.add(cost_value)\n",
    "            if PLOTTING_SUPPORT:\n",
    "              progress_bar(val_batchmaker)\n",
    "        print(\"Validation cost: \"+str(avg_val_cost)+\"  (Training cost: \"+str(avg_step_cost)+\")\", end=\"\")\n",
    "        try:\n",
    "          print(\" Step Time: \" + str(step_end-step_start))\n",
    "          if DETAILED_STEP_TIMES:\n",
    "            print(step_times)\n",
    "        except: \n",
    "            print(\" \")\n",
    "\n",
    "        val_cost_log.append(avg_val_cost.values)\n",
    "\n",
    "\n",
    "        if PLOTTING_SUPPORT:\n",
    "          # Plot a few random samples\n",
    "          import matplotlib.pyplot as plt\n",
    "          n_samples = 1\n",
    "          import random\n",
    "          x_samples = random.sample(val_vox, n_samples)\n",
    "          x_samples = [np.reshape(sample, MP.INPUT_SHAPE) for sample in x_samples]\n",
    "          x_reconstruct = list(vae.encode_decode(x_samples))\n",
    "          try:\n",
    "            sample_history += x_samples\n",
    "            recons_history += x_reconstruct\n",
    "          except NameError:\n",
    "            sample_history = x_samples\n",
    "            recons_history = x_reconstruct\n",
    "          for i, (in_, out_) in enumerate(zip(sample_history, recons_history)):\n",
    "            plt.figure(\"reconstruction for step \"+str(i), figsize=(8, 4))\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.imshow(in_.reshape(VOXEL_SIDE, VOXEL_SIDE*VOXEL_SIDE), vmin=0, vmax=1, cmap='spectral')\n",
    "            plt.title(\"Top: val input - Bottom: Reconstruction\")\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.imshow(out_.reshape(VOXEL_SIDE, VOXEL_SIDE*VOXEL_SIDE), vmin=0, vmax=1, cmap='spectral')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.gcf().canvas.draw()\n",
    "\n",
    "        # Training Monitor\n",
    "        if len(val_cost_log) > 1:\n",
    "            # Save cost log.\n",
    "            import os\n",
    "            if not os.path.exists(SAVE_DIR):\n",
    "                os.makedirs(SAVE_DIR)\n",
    "                print(\"Created directory: %s\" % SAVE_DIR)\n",
    "                with open(SAVE_DIR+MP_FILENAME, 'wb') as file:\n",
    "                  pickle.dump(MP, file, protocol=2)\n",
    "            np.savetxt(SAVE_DIR+\"val_cost_log.txt\", val_cost_log)\n",
    "            # Save if cost has improved. Otherwise increment counter.\n",
    "            if np.less(val_cost_log[-1], np.min(val_cost_log[:-1], 0))[0]:\n",
    "                val_steps_since_last_improvement = 0\n",
    "                # save model to disk\n",
    "                print(\"Saving ... \", end='')\n",
    "                save_path = vae.saver.save(vae.sess, SAVE_PATH)\n",
    "                print(\"Model saved in file: %s\" % save_path)      \n",
    "            else:\n",
    "                val_steps_since_last_improvement += 1  \n",
    "        # Stop training if val_cost hasn't improved in VAL_STEP_TOLERANCE steps\n",
    "        if val_steps_since_last_improvement > VAL_STEP_TOLERANCE:\n",
    "            print(\"Training stopped by validation monitor.\")\n",
    "            break\n",
    "\n",
    "      # Train on batches\n",
    "      step_start = timer()\n",
    "      zero = timer() - timer()\n",
    "      step_times = {'batchmaking': zero, 'training': zero, 'plotting': zero}\n",
    "      avg_step_cost = Average()\n",
    "      training_batchmaker = Batchmaker(train_vox, BATCH_SIZE, MP)\n",
    "      if MP.ADVERSARIAL:\n",
    "        train_order = 4*[vae.optimizer] + 4*[vae.generator_optimizer] + [vae.discriminator_optimizer]\n",
    "      elif MP.MUTUAL_INFO:\n",
    "        train_order = [vae.optimizer, vae.optimizer_with_MI]\n",
    "      else:\n",
    "        train_order = [vae.optimizer]\n",
    "      for train_target in itertools.cycle(train_order):\n",
    "        if MP.ADVERSARIAL:\n",
    "          if train_target is vae.discriminator_optimizer:\n",
    "            if cost_value[1] > G_THRESHOLD or cost_value[2] < D_THRESHOLD:\n",
    "              continue\n",
    "        if training_batchmaker.is_depleted():\n",
    "          break\n",
    "        else:\n",
    "          t_a = timer()  \n",
    "          batch_input_values = training_batchmaker.next_batch()\n",
    "          t_b = timer()\n",
    "          # Train over 1 batch.\n",
    "          cost_value = vae.train_on_single_batch(batch_input_values, train_target=train_target, summary_writer=summary_writer)\n",
    "          avg_step_cost.add(cost_value)\n",
    "          t_c = timer()\n",
    "          if PLOTTING_SUPPORT:\n",
    "            progress_bar(training_batchmaker)\n",
    "          t_d = timer()\n",
    "          step_times['batchmaking'] += t_b - t_a\n",
    "          step_times['training']    += t_c - t_b\n",
    "          step_times['plotting']    += t_d - t_c\n",
    "      step_cost_log.append(avg_step_cost.values)\n",
    "      step_end = timer()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted.\")\n",
    "else:\n",
    "    print(\"Training ended.\")\n",
    "    \n",
    "if SAVE_UNVALIDATED:\n",
    "    SAVE_PATH_NOVAL = SAVE_DIR_NOVAL+SAVE_FILE\n",
    "    try: \n",
    "        os.makedirs(SAVE_DIR_NOVAL)\n",
    "        print(\"Directory created\", SAVE_DIR_NOVAL)\n",
    "    except:\n",
    "        print(\"\")\n",
    "    print(\"Saving ... \", end='')\n",
    "    save_path = vae.saver.save(vae.sess, SAVE_PATH_NOVAL)\n",
    "    print(\"Unvalidated model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if CREATE_VISUALS:\n",
    "  raise ValueError('Training ended.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Autoencoder Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  # Plot a few random samples\n",
    "  import matplotlib.pyplot as plt\n",
    "  %matplotlib notebook\n",
    "  plt.ion()\n",
    "  n_samples = 5\n",
    "  import random\n",
    "  x_samples = random.sample(val_vox, 5)\n",
    "  x_samples = [np.reshape(sample, MP.INPUT_SHAPE) for sample in x_samples]\n",
    "  x_reconstruct = vae.encode_decode(x_samples)\n",
    "  plt.figure(figsize=(8, 12))\n",
    "  for i in range(n_samples):\n",
    "    plt.subplot(n_samples*2, 1, 2*i + 1)\n",
    "    plt.imshow(x_samples[i].reshape(VOXEL_SIDE, VOXEL_SIDE*VOXEL_SIDE), vmin=0, vmax=1, cmap='spectral')\n",
    "    plt.title(\"Top: val input - Bottom: Reconstruction\")\n",
    "    plt.subplot(n_samples*2, 1, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[i].reshape(VOXEL_SIDE, VOXEL_SIDE*VOXEL_SIDE), vmin=0, vmax=1, cmap='spectral')\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  nx = ny = 4\n",
    "  nz = 1\n",
    "  dim1 = 0\n",
    "  dim2 = 1\n",
    "  dim3 = 0\n",
    "  x_values = np.linspace(-3, 3, nx)\n",
    "  y_values = np.linspace(-3, 3, ny)\n",
    "  z_values = np.linspace(-3, 3, nz)\n",
    "  canvas = np.empty((VOXEL_SIDE*ny, VOXEL_SIDE*nx, VOXEL_SIDE*nz))\n",
    "  for i, yi in enumerate(x_values):\n",
    "      for j, xi in enumerate(y_values):\n",
    "          for k, zi in enumerate(z_values):\n",
    "              # we can only visualize 3 dimensions, in this case the first 3\n",
    "              latent_sample = np.zeros([1]+MP.LATENT_SHAPE)\n",
    "              latent_sample.flat[dim1] = xi\n",
    "              latent_sample.flat[dim2] = yi\n",
    "              latent_sample.flat[dim3] = zi\n",
    "              x_mean = vae.decode(latent_sample)\n",
    "              canvas[(nx-i-1)*VOXEL_SIDE:(nx-i)*VOXEL_SIDE,\n",
    "                     j*VOXEL_SIDE:(j+1)*VOXEL_SIDE,\n",
    "                     k*VOXEL_SIDE:(k+1)*VOXEL_SIDE] \\\n",
    "                   = x_mean[0].reshape(VOXEL_SIDE, VOXEL_SIDE, VOXEL_SIDE)\n",
    "  from mpl_toolkits.mplot3d import Axes3D\n",
    "  threshold = 0.7\n",
    "  X,Y,Z = np.where(canvas > (threshold*np.max(canvas)))\n",
    "  fig = plt.figure()\n",
    "  plt.cla()\n",
    "  ax = Axes3D(fig)\n",
    "  ax.scatter(X, Y, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Autoencoder Features for Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  print(\"Voxelizing segments\")\n",
    "  from voxelize import voxelize\n",
    "  segments_vox, features_voxel_scale = voxelize(segments, VOXEL_SIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "    print(\"Computing Eigenvalue Features\")\n",
    "    from eigenvalues import eigenvalue_features\n",
    "    features_eig = eigenvalue_features(segments)\n",
    "    features_eig[np.where(np.isnan(features_eig))] = 0\n",
    "    F = features_eig\n",
    "    C = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  print(\"Computing Features for Segments\")\n",
    "  features_nn, confusion_nn = vae.batch_encode([np.reshape(sample, MP.INPUT_SHAPE) for sample in segments_vox])\n",
    "  fnames_nn = ['autoencoder_feature'+str(i+1) for i, _ in enumerate(features_nn[0])]\n",
    "  F = features_nn\n",
    "  C = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  print(\"Rotating segments\")\n",
    "  from voxelize import create_rotations\n",
    "  rotated_segments, rotated_classes = create_rotations(segments, N_ROTATION_ANGLES, classes=classes)\n",
    "  if False: # walls_vs_cars\n",
    "    print(\"Removing unknowns\")\n",
    "    rotated_segments = [segment for segment, class_ in zip(rotated_segments, rotated_classes) if class_ != \"unknown\"]\n",
    "    rotated_classes = [class_ for class_ in rotated_classes if class_ != \"unknown\"]\n",
    "  print(\"Voxelizing rotations\")\n",
    "  from voxelize import voxelize\n",
    "  rotated_segments_vox, rotated_segments_scale = voxelize(rotated_segments, VOXEL_SIDE)\n",
    "  print(\"Computing Features for rotations\")\n",
    "  rotated_features, _ = vae.batch_encode([np.reshape(sample, MP.INPUT_SHAPE) for sample in rotated_segments_vox])\n",
    "  F = rotated_features\n",
    "  C = rotated_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  print(\"T-SNE\")\n",
    "  dir_ = \"/tmp/online_matcher/visuals/\"\n",
    "  import os\n",
    "  if not os.path.exists(dir_):\n",
    "    os.makedirs(dir_)\n",
    "  if MP.LATENT_SHAPE[0] == 2:\n",
    "    F2 = F\n",
    "  else:\n",
    "    from tools.tsne import tsne\n",
    "    F2 = tsne(F, err_threshold=1.0)\n",
    "  from itertools import cycle\n",
    "  cnames = ['dodgerblue', 'gold', 'silver', 'tomato', \n",
    "            'plum', 'lemonchiffon', 'grey', 'orchid', 'lime', 'palegreen']\n",
    "  from matplotlib import pyplot as plt\n",
    "  plt.figure(figsize=(12,7))\n",
    "  for c_, name in zip(cycle(cnames), classes_set):\n",
    "    x = [values[0] for values, class_ in zip(F2, C) if class_ == name]\n",
    "    y = [values[1] for values, class_ in zip(F2, C) if class_ == name]\n",
    "    plt.scatter(x, y, c=c_, alpha=0.8,  lw = 0)\n",
    "  box = plt.gca().get_position()\n",
    "  plt.gca().set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "  ncol = 2 if len(classes_set) > 10 else 1\n",
    "  plt.legend(classes_set, loc='center left', bbox_to_anchor=(1, 0.5), ncol=ncol)\n",
    "  plt.title('T-SNE')\n",
    "  plt.xlabel('x_dim')\n",
    "  plt.ylabel('y_dim')\n",
    "  plt.show()\n",
    "  try:\n",
    "    plt.gcf().savefig(dir_+\"t-sne.png\")\n",
    "  except:\n",
    "    print(\"not saved.\")\n",
    "  if len(matches) > 0:\n",
    "    print(\"Adding matches\")\n",
    "    # Dim all points\n",
    "    plt.cla()\n",
    "    for c_, name in zip(cycle(cnames), classes_set):\n",
    "      x = [values[0] for values, class_ in zip(F2, C) if class_ == name]\n",
    "      y = [values[1] for values, class_ in zip(F2, C) if class_ == name]\n",
    "      plt.scatter(x, y, c=c_, alpha=0.2,  lw = 0)\n",
    "    plt.legend(classes_set, loc='center left', bbox_to_anchor=(1, 0.5), ncol=ncol)\n",
    "    plt.title('T-SNE')\n",
    "    plt.xlabel('x_dim')\n",
    "    plt.ylabel('y_dim')\n",
    "    # Bring out matched points\n",
    "    matched_ids = [id_ for match in matches for id_ in match]\n",
    "    for c_, name in zip(cycle(cnames), classes_set):\n",
    "      x = [values[0] for values, class_, id_ in zip(F2, C, ids) if class_ == name and id_ in matched_ids]\n",
    "      y = [values[1] for values, class_, id_ in zip(F2, C, ids) if class_ == name and id_ in matched_ids]\n",
    "      plt.scatter(x, y, c=c_, s=30, lw = 1)\n",
    "    # Show matches as lines\n",
    "    for match in matches:\n",
    "        line_x = [ F2[ids.index(match[0])][0], F2[ids.index(match[1])][0] ]\n",
    "        line_y = [ F2[ids.index(match[0])][1], F2[ids.index(match[1])][1] ]\n",
    "        plt.plot(line_x, line_y, 'black', linewidth=1)\n",
    "        try:\n",
    "            plt.gcf().savefig(dir_+\"t-sne_matches.png\")\n",
    "        except:\n",
    "            print(\"not saved.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RC_CONFIDENCE = 0.1\n",
    "ONEVIEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reconstructions\n",
    "if not RUN_AS_PY_SCRIPT:\n",
    "  N = 100\n",
    "  SV_ = segments_vox[:N]\n",
    "  S_ = segments[:N]\n",
    "  I_ = ids[:N]\n",
    "  reconstruction_vox = vae.batch_encode_decode([np.reshape(sample, MP.INPUT_SHAPE) for sample in SV_])\n",
    "  reconstruction_vox = [np.reshape(vox, [VOXEL_SIDE, VOXEL_SIDE, VOXEL_SIDE]) for vox in reconstruction_vox]\n",
    "  from voxelize import unvoxelize\n",
    "  reconstruction = [unvoxelize(vox > RC_CONFIDENCE) for vox in reconstruction_vox]\n",
    "  reconstruction = [segment*scale for (segment, scale) in zip(reconstruction, features_voxel_scale)]             \n",
    "  if CREATE_VISUALS:\n",
    "    dir_ = \"/tmp/online_matcher/visuals/reconstructions/\"\n",
    "    from visuals import visuals_of_matches\n",
    "    reconstruction_ids = [id_+max(I_)+1 for id_ in I_]\n",
    "    one_to_one_matches = [[id1, id2] for id1, id2 in zip(I_, reconstruction_ids)]\n",
    "    visuals_of_matches(one_to_one_matches, S_+reconstruction, I_+reconstruction_ids, directory=dir_, oneview=ONEVIEW)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exploring influence of first dimension on generated segments\n",
    "dim_ = 0\n",
    "if CREATE_VISUALS:\n",
    "  # Use a pre-existing segment as starting point\n",
    "  dir_ = \"/tmp/online_matcher/visuals/gen_rotations/\"\n",
    "  class_name = \"car\"\n",
    "  id_ = np.random.choice([id_ for id_, class_ in zip(ids, classes) if class_ == class_name])\n",
    "  class_segments = [np.array(segments)[ids.index(id_)]]\n",
    "  from voxelize import voxelize\n",
    "  class_segments_vox, class_voxel_scale = voxelize(class_segments, VOXEL_SIDE)\n",
    "  code, _ = vae.batch_encode([np.reshape(vox, MP.INPUT_SHAPE) for vox in class_segments_vox])\n",
    "  code = code[0]\n",
    "  \n",
    "  values = [code[dim_]] + list(np.linspace(-10.,10.,10))\n",
    "  gen_codes = []\n",
    "  for value in values:\n",
    "    gen_code = code\n",
    "    gen_code[dim_] = value\n",
    "    gen_codes.append(gen_code)\n",
    "  gen_ids = range(len(gen_codes))\n",
    "  \n",
    "  gen_segments_vox = vae.batch_decode(gen_codes)\n",
    "  gen_segments_vox = [np.reshape(vox, [VOXEL_SIDE, VOXEL_SIDE, VOXEL_SIDE]) for vox in gen_segments_vox]\n",
    "  from voxelize import unvoxelize\n",
    "  gen_segments = [unvoxelize(vox > RC_CONFIDENCE) for vox in gen_segments_vox]\n",
    "  from visuals import visuals_of_segments\n",
    "  visuals_of_segments(gen_segments, gen_ids, directory=dir_, oneview=ONEVIEW)\n",
    "  clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reconstructions of rotations for one object\n",
    "if CREATE_VISUALS:\n",
    "  dir_ = \"/tmp/online_matcher/visuals/rotations/\"\n",
    "  class_name = \"car\"\n",
    "  class_ids = [np.random.choice([id_ for id_, class_ in zip(ids, classes) if class_ == class_name])]\n",
    "  class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "  class_segments = np.array(segments)[class_indices]\n",
    "  \n",
    "  from voxelize import create_rotations\n",
    "  class_rotated_segments = np.array(list(class_segments) + list(create_rotations(class_segments, N_ROTATION_ANGLES)))\n",
    "  from voxelize import voxelize\n",
    "  class_segments_vox, class_voxel_scale = voxelize(class_rotated_segments, VOXEL_SIDE)\n",
    "  if CREATE_VISUALS:\n",
    "    class_reconstruction_vox = vae.batch_encode_decode([np.reshape(vox, MP.INPUT_SHAPE) for vox in class_segments_vox])\n",
    "    class_reconstruction_vox = [np.reshape(vox, [VOXEL_SIDE, VOXEL_SIDE, VOXEL_SIDE]) for vox in class_reconstruction_vox]\n",
    "    from voxelize import unvoxelize\n",
    "    class_reconstruction = [unvoxelize(vox > RC_CONFIDENCE) for vox in class_reconstruction_vox]\n",
    "    class_reconstruction = [segment*scale for (segment, scale) in zip(class_reconstruction, class_voxel_scale)] \n",
    "    from visuals import visuals_of_matches\n",
    "    fake_ids = list(range(len(class_reconstruction)))\n",
    "    fake_reconstruction_ids = [id_+max(fake_ids)+1 for id_ in fake_ids]\n",
    "    one_to_one_matches = [[id1, id2] for id1, id2 in zip(fake_ids, fake_reconstruction_ids)]\n",
    "    visuals_of_matches(one_to_one_matches,\n",
    "                       list(class_rotated_segments)+class_reconstruction,\n",
    "                       fake_ids+fake_reconstruction_ids,\n",
    "                       directory=dir_, oneview=ONEVIEW)\n",
    "    clear_output()\n",
    "  class_features, confusion = vae.batch_encode([np.reshape(vox, MP.INPUT_SHAPE) for vox in class_segments_vox])\n",
    "  class_features = np.array(class_features)\n",
    "  print(class_name)\n",
    "  print(\"Id: \"+str(class_ids[0]))\n",
    "  from matplotlib import pyplot as plt\n",
    "  plt.figure()\n",
    "  plt.step(range(len(class_features.T)), class_features.T, color='k', alpha=0.2, where='mid')\n",
    "  plt.plot(np.sqrt(np.exp(confusion)).T, 'r')\n",
    "  plt.show()\n",
    "  plt.gcf().savefig(dir_+\"signature.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Gifs\n",
    "if CREATE_VISUALS:\n",
    "    id_ = np.random.choice(ids)\n",
    "    print(id_)\n",
    "    segment = segments[ids.index(id_)]\n",
    "    import visuals\n",
    "    visuals.single_segment_as_gif(segment)\n",
    "    visuals.single_segment_reconstruction_as_gif(segment, vae, confidence=0.3)\n",
    "    visuals.single_segment_rotations_reconstruction_as_gif(segment, vae, confidence=0.3)\n",
    "    visuals.single_segment_degeneration_as_gif(segment, vae, confidence=0.3)\n",
    "    visuals.single_segment_confidence_as_gif(segment, vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  dir_ = \"/tmp/online_matcher/visuals/reconstructions/\"\n",
    "  for class_name in classes_set:\n",
    "    print(class_name)\n",
    "    class_ids = [id_ for id_, class_ in zip(ids, classes) if class_ == class_name]\n",
    "    class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "    class_segments = np.array(segments)[class_indices]\n",
    "    class_features = np.array(features_nn)[class_indices]\n",
    "    class_confusion = np.array(confusion_nn)[class_indices]\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.step(range(len(class_features.T)), class_features.T, color='k', alpha=0.2, where='mid')\n",
    "    plt.plot(np.sqrt(np.exp(class_confusion)).T, 'r')\n",
    "    plt.show()\n",
    "    plt.gcf().savefig(dir_+class_name+\"_signature.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  # Include Rotated segments\n",
    "  for class_name in classes_set:\n",
    "    print(class_name)\n",
    "    class_ids = [id_ for id_, class_ in zip(ids, classes) if class_ == class_name]\n",
    "    class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "    class_segments = np.array(segments)[class_indices]\n",
    "  \n",
    "    from voxelize import create_rotations\n",
    "    class_rotated_segments = np.array(list(class_segments) + list(create_rotations(class_segments, N_ROTATION_ANGLES)))\n",
    "    from voxelize import voxelize\n",
    "    class_segments_vox, _ = voxelize(class_rotated_segments, VOXEL_SIDE)\n",
    "    class_features, confusion = vae.batch_encode([np.reshape(vox, MP.INPUT_SHAPE) for vox in class_segments_vox])\n",
    "    class_features = np.array(class_features)\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.step(range(len(class_features.T)), class_features.T, color='k', alpha=0.2, where='mid')\n",
    "    plt.plot(np.sqrt(np.exp(confusion)).T, 'r')\n",
    "    plt.show()\n",
    "    plt.gcf().savefig(dir_+class_name+\"_rotations_signature.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  from itertools import cycle\n",
    "  colors = cycle(['dodgerblue', 'gold', 'silver', 'tomato'])\n",
    "  plt.figure()\n",
    "  plt.title(\"Average absolute value of features, per class\")\n",
    "  plt.xlabel('feature #')\n",
    "  plt.ylabel('avg(abs(feature))')\n",
    "  for class_name, color_ in zip(classes_set, colors):\n",
    "    class_ids = [id_ for id_, class_ in zip(ids, classes) if class_ == class_name]\n",
    "    class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "    class_features = np.array(features_nn)[class_indices]\n",
    "    plt.plot(np.mean(np.abs(class_features), axis=0), marker='_', color=color_, label=class_name)\n",
    "    plt.hlines(np.mean(np.abs(class_features)),0,len(class_features[0])-1, linestyle='--', color=color_)\n",
    "  plt.show()\n",
    "  plt.legend()\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.title(\"Average confusion, per class\")\n",
    "  plt.xlabel('feature #')\n",
    "  plt.ylabel('sigma^2')\n",
    "  for class_name, color_ in zip(classes_set, colors):\n",
    "    class_ids = [id_ for id_, class_ in zip(ids, classes) if class_ == class_name]\n",
    "    class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "    class_confusion = np.array(confusion_nn)[class_indices]\n",
    "    plt.plot(np.mean(np.exp(class_confusion), axis=0), marker='_', color=color_, label=class_name)\n",
    "    plt.hlines(np.mean(np.exp(class_confusion)),0,len(class_features[0])-1, linestyle='--', color=color_)\n",
    "  plt.show()\n",
    "  plt.legend()\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_features(fnames_to_remove, fnames, features):\n",
    "    # Remove the autencoder features from the imported features if they already exist\n",
    "    for fname_to_remove in fnames_to_remove:\n",
    "        if fname_to_remove in fnames:\n",
    "            print(\"  Removing pre-existing feature \" + fname_to_remove)\n",
    "            for j, values in enumerate(features):\n",
    "                features[j] = np.delete(values, fnames.index(fname_to_remove))\n",
    "            fnames.remove(fname_to_remove)\n",
    "\n",
    "    assert len(fnames) == len(features[0])\n",
    "    \n",
    "def update_features(fnames_to_update, features_to_update, fnames, features):\n",
    "    assert len(fnames_to_update) == len(features_to_update[0])\n",
    "    # Remove the selected features if they already exist\n",
    "    remove_features(fnames_to_update, fnames, features)\n",
    "    # Add in the selected features\n",
    "    for fname in fnames_to_update: print(\"  Adding feature \" + fname)\n",
    "    for i, [f, ftu] in enumerate(zip(features, features_to_update)):\n",
    "        features[i] = np.concatenate([f, ftu])\n",
    "    fnames += fnames_to_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create copies of the original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if EXPORT_FEATURES:\n",
    "  updated_fnames = fnames[:]\n",
    "  updated_features = features[:]\n",
    "\n",
    "  print(fnames)\n",
    "  print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add/overwrite autoencoder features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if EXPORT_FEATURES:\n",
    "  # AE features\n",
    "  fnames_nn = ['autoencoder_feature'+str(i+1) for i in range(features_nn[0].shape[0])]\n",
    "  update_features(fnames_nn, features_nn, updated_fnames, updated_features)\n",
    "\n",
    "  # Scale features\n",
    "  sc_fnames = ['scale_x', 'scale_y', 'scale_z']\n",
    "  update_features(sc_fnames, features_voxel_scale, updated_fnames, updated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if EXPORT_FEATURES:\n",
    "  from load_segments import write_features\n",
    "  write_features(ids, updated_features, updated_fnames, filename=runs[run_index][features_file_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Features\n",
    "if CREATE_VISUALS:\n",
    "  from visuals import visuals_of_segments\n",
    "  visuals_of_segments(segments, ids, features=features_nn)\n",
    "  clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matches\n",
    "if CREATE_VISUALS:\n",
    "  from visuals import visuals_of_matches\n",
    "  visuals_of_matches(matches, segments, ids, features=features_nn)\n",
    "  clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Convert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONVERT_VARIABLE_NAMES = False\n",
    "name_to_var_dict = {}\n",
    "if CONVERT_VARIABLE_NAMES:\n",
    "  for var in vae.variables:\n",
    "    # Modify a few names\n",
    "    if 'LatentLayerWeights/' in var.name:\n",
    "      name = var.name.replace('LatentLayerWeights/', '')\n",
    "    elif 'ReconstructionLayerWeights/' in var.name:\n",
    "      name = var.name.replace('ReconstructionLayerWeights/', '')\n",
    "    # Leave other names unchanged\n",
    "    else:\n",
    "      name = var.name\n",
    "    name_to_var_dict[name] = var\n",
    "  temp_saver = tf.train.Saver(name_to_var_dict)\n",
    "  temp_saver.restore(vae.sess, SAVE_PATH)\n",
    "name_to_var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model and params\n",
    "if False:\n",
    "  vae.saver.save(vae.sess, SAVE_PATH)\n",
    "  with open(SAVE_DIR+MP_FILENAME, 'wb') as file:\n",
    "    pickle.dump(MP, file, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ml3]",
   "language": "python",
   "name": "Python [ml3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {
    "bb4397bb11c84525a852b17aaabffdc4": {
     "views": []
    },
    "bd44887d37014fcdb93112c4a9578106": {
     "views": []
    },
    "d279e0ce71ec4cbda4c2f2c8e24d1b96": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "e76e17549893445980c0385a1c4177d9": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
